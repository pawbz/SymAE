
"""
Returns data iterator `X` where each datapoint is generated by randomly picking  `ntau` instances from a ticular state.
During training, it is very important to mix instances from different states in each batch. In other words, a small batch size, e.g., `1`, is observed not to disentangle coherent from nuisance information. Therefore, it is recommended to set the batch size as high as possible.

* `nd` : number of output datapoints
* `ntau` : number of instances in each datapoint (typically 20)
* `dvec` : a vector where the elements are measured instances from different states
* `batchsize` : number of data points used in each batch for training
"""
function get_data_iterator(dvec, nd = 1000, batchsize = 256, ntau = 20)
    drepeat = Flux.stack([randobs(randobs(dvec), ntau) for i = 1:nd], dims = 3)
    return BatchView(drepeat, batchsize = batchsize)
end

## get networks
function get_dense_networks(nt, p, q)
    l2p = floor.(Int, LinRange(nt, 2 * p, (5,)))
    lq = floor.(Int, LinRange(nt, q, (5,)))
    lpq = floor.(Int, LinRange(p + q, nt, (5,)))
    senc1 =
        Chain(
            Dense(nt, l2p[2], elu),
            Dense(l2p[2], l2p[3], elu),
            Dense(l2p[3], l2p[4], elu),
            Dense(l2p[4], 2 * p),
        ) |> xpu
    senc2 = Chain(Dense(2 * p, 2 * p, elu), Dense(2 * p, p)) |> xpu

    nenc =
        Chain(
            Dense(nt, lq[2], elu),
            Dense(lq[2], lq[3], elu),
            Dense(lq[3], lq[4], elu),
            Dense(lq[4], q),
            Dropout(0.5, dims = (1, 2, 3)),
        ) |> xpu

    dec =
        Chain(
            Dense(p + q, lpq[2], elu),
            Dense(lpq[2], lpq[3], elu),
            Dense(lpq[3], lpq[4], elu),
            Dense(lpq[4], nt),
        ) |> xpu
    return @strdict senc1 senc2 nenc dec
end
##

function get_conv_networks(nt, p, q)
    senc1 = Chain(
        x -> reshape(x, size(x, 1), 1, size(x, 2)),
        Conv((5,), 1 => 64, elu, ; pad = SamePad()),
        Conv((5,), 64 => 64, elu, ; pad = SamePad()),
        MaxPool((2,)),
        Conv((5,), 64 => 64, elu, ; pad = SamePad()),
        Conv((5,), 64 => 64, elu, ; pad = SamePad()),
        MaxPool((2,)),
    ) |> xpu

    senc2 = Chain(
        Conv((5,), 64 => 32, elu, ; pad = SamePad()),
        MaxPool((2,)),
        Conv((5,), 32 => 1, ; pad = SamePad()),
        BatchNorm(1, elu),
        MaxPool((2,)),
        x -> Flux.flatten(x),
        Dense(div(nt, 16), p),
    )|> xpu


    # 1D convolutional layers [time, channel, batch]
    nenc = Chain(
        x -> reshape(x, size(x, 1), 1, size(x, 2)),
        # Dropout(0.3, dims = (1, 2, 3)),
        Conv((5,), 1 => 64, elu, ; pad = SamePad()),
        Conv((5,), 64 => 64, elu, ; pad = SamePad()),
        # MaxPool((2,)),
        Dropout(0.4, dims = (1, 2, 3)),
        Conv((5,), 64 => 64, elu, ; pad = SamePad()),
        Conv((5,), 64 => 32, elu, ; pad = SamePad()),
        Dropout(0.4, dims = (1, 2, 3)),
        MaxPool((2,)),
        Conv((5,), 32 => 32, elu, ; pad = SamePad()),
        Conv((5,), 32 => 32, elu, ; pad = SamePad()),
        # MaxPool((2,)),
        Dropout(0.4, dims = (1, 2, 3)),
        Conv((5,), 32 => 8, elu, ; pad = SamePad()),
        Conv((5,), 8 => 1, ; pad = SamePad()),
        BatchNorm(1, elu),
        # MaxPool((2,)),
        x -> Flux.flatten(x),
        Dense(div(nt, 2), q),
        Dropout(0.4, dims = (1, 2, 3)),
    )|> xpu



    dec = Chain(
        Dense(p + q, div(nt, 8), elu),
        x -> reshape(x, size(x, 1), 1, size(x, 2)),
        Conv((5,), 1 => 64, elu, ; pad = SamePad()),
        Upsample(8),
        Conv((5,), 64 => 64, elu, ; pad = SamePad()),
        Conv((5,), 64 => 64, ; pad = SamePad()),
        BatchNorm(64, elu),
        Conv((5,), 64 => 64, elu, ; pad = SamePad()),
        Conv((5,), 64 => 64, elu, ; pad = SamePad()),
        Conv((5,), 64 => 1, ; pad = SamePad()),
    )|> xpu

    return @strdict senc1 senc2 nenc dec
end

struct BroadcastSenc
    senc1::Chain
    senc2::Chain
end
struct BroadcastNenc
    chain::Chain
end
struct BroadcastDec
    chain::Chain
end
struct JoinEncs{T1,T2}
    senc::T1
    nenc::T2
end

function (m::BroadcastSenc)(x)
    x = cat(x, dims = 3)
    n = size(x)

    X = reshape(x, n[1:end-2]..., n[end-1] * n[end])
    X = m.senc1(X)
    n1 = size(X)
    X = reshape(X, n1[1:end-1]..., n[end-1], n[end])
    X = mean(X, dims = ndims(X) - 1)
    X = dropdims(X, dims = ndims(X) - 1)
    X = m.senc2(X)
    X = Flux.stack(fill(X, n[end-1]), dims=length(n) - 1)
    return X
end
function (m::BroadcastNenc)(x)
    x = cat(x, dims = 3)
    n1, n2, n3 = size(x)
    X = reshape(x, :, n2 * n3)
    X = m.chain(X)
    X = reshape(X, :, n2, n3)
    return X
end
function (m::BroadcastDec)(x)
    x = cat(x, dims = 3)
    n1, n2, n3 = size(x)
    X = reshape(x, :, n2 * n3)
    X = m.chain(X)
    X = reshape(X, :, n2, n3)
    return X
end
function (m::JoinEncs)(x)
    return cat(m.senc(x), m.nenc(x), dims = 1)
end
Flux.@functor BroadcastSenc
Flux.@functor BroadcastNenc
Flux.@functor BroadcastDec
Flux.@functor JoinEncs


